{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/get-started/locally/\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNveqeA1KXGy"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kTvDNSILZoN9",
    "outputId": "48eaf63b-1591-4223-862b-fc08da08645e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'yolov5'\n",
      "c:\\Users\\AmirH Far\\Desktop\\dev\\camAIv5\\yolov5\n",
      "Setup complete. Using torch 2.7.1+cu128 (NVIDIA GeForce RTX 3060 Ti)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "%cd yolov5\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train YOLOv5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AmirH Far\\Desktop\\dev\\camAIv5\\yolov5\\utils\\general.py:32: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources as pkg\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=../fire.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=3, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data\\hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=1, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 \n",
      "fatal: cannot change to 'C:\\Users\\AmirH': Invalid argument\n",
      "YOLOv5  2025-7-11 Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8191MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\n",
      "\n",
      "Dataset not found , missing paths ['C:\\\\Users\\\\AmirH Far\\\\Desktop\\\\dev\\\\camAIv5\\\\yolov5\\\\obj\\\\images\\\\val']\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"c:\\Users\\AmirH Far\\Desktop\\dev\\camAIv5\\yolov5\\train.py\"\u001b[0m, line \u001b[35m988\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[31mmain\u001b[0m\u001b[1;31m(opt)\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\Users\\AmirH Far\\Desktop\\dev\\camAIv5\\yolov5\\train.py\"\u001b[0m, line \u001b[35m690\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mtrain\u001b[0m\u001b[1;31m(opt.hyp, opt, device, callbacks)\u001b[0m\n",
      "    \u001b[31m~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\Users\\AmirH Far\\Desktop\\dev\\camAIv5\\yolov5\\train.py\"\u001b[0m, line \u001b[35m205\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "    data_dict = data_dict or \u001b[31mcheck_dataset\u001b[0m\u001b[1;31m(data)\u001b[0m  # check if None\n",
      "                             \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\Users\\AmirH Far\\Desktop\\dev\\camAIv5\\yolov5\\utils\\general.py\"\u001b[0m, line \u001b[35m565\u001b[0m, in \u001b[35mcheck_dataset\u001b[0m\n",
      "    raise Exception(\"Dataset not found ❌\")\n",
      "\u001b[1;35mException\u001b[0m: \u001b[35mDataset not found ❌\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 640 --batch 16 --epochs 3 --data ../fire.yaml --weights yolov5s.pt --workers 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcIRLQOlA14A"
   },
   "source": [
    "# Evaluate Custom YOLOv5 Detector Performance\n",
    "Training losses and performance metrics are saved to Tensorboard and also to a logfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plots import plot_results\n",
    "plot_results('runs/train/exp/results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.25 --source ../datasets/fire/val/images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZbUn4_b9GCKO",
    "outputId": "70f601b7-d53d-4efd-fb97-6ae373b15b9f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#display inference on some test images\n",
    "images = glob.glob('runs/detect/exp/*.jpg')\n",
    "\n",
    "for imageName in images[:3]: #assuming JPG\n",
    "    display(Image(filename=imageName, width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNn-obvOGITm"
   },
   "source": [
    "### Prediction on Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.25 --source ../input.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "vidcap = cv2.VideoCapture('runs/detect/exp2/input.mp4')\n",
    "success,image = vidcap.read()\n",
    "images = []\n",
    "while success:\n",
    "    success,image = vidcap.read()\n",
    "    if success:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "\n",
    "def create_animation(ims):\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "    plt.axis('off')\n",
    "    im = plt.imshow(ims[0])\n",
    "\n",
    "    def animate_func(i):\n",
    "        im.set_array(ims[i])\n",
    "        return [im]\n",
    "\n",
    "    return animation.FuncAnimation(fig, animate_func, frames = len(ims), interval = 1000//12)\n",
    "\n",
    "create_animation(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../datasets/fire/val/images/004dec94c5de631f.jpg\"\n",
    "display(Image(filename=image_path, width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.25 --source {image_path} --visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=\"runs/detect/exp3/004dec94c5de631f/stage23_C3_features.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "YOLOv5-Custom-Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
